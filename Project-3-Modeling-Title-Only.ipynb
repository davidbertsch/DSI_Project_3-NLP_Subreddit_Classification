{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = pd.read_csv(\"./dead.csv\")\n",
    "phish = pd.read_csv(\"./phish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead.drop(columns = [col for col in dead.columns if col not in [\"title\", \"subreddit\"]],\n",
    "          inplace=True)\n",
    "\n",
    "phish.drop(columns = [col for col in phish.columns if col not in [\"title\", \"subreddit\"]],\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = dead.merge(phish, how=\"outer\")\n",
    "jam[\"subreddit\"] = jam[\"subreddit\"].map({\"gratefuldead\": 0,\n",
    "                                         \"phish\": 1})\n",
    "jam.rename(columns = {\"title\": \"text\"}, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 7227\n",
      "\n",
      "training accuracy score: 0.9497251488776912\n",
      " testing accuracy score: 0.8210237031947785\n"
     ]
    }
   ],
   "source": [
    "X = jam[\"text\"]\n",
    "y = jam[\"subreddit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cv, y_train)\n",
    "\n",
    "print(f\"number of features: {len(cv.get_feature_names())}\")\n",
    "print(\"\")\n",
    "print(f\"\"\"training accuracy score: {lr.score(X_train_cv, y_train)}\n",
    " testing accuracy score: {lr.score(X_test_cv, y_test)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(r\"\\[deleted\\]\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(r\"\\[removed\\]\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_url(text):\n",
    "    text_list = text.split()\n",
    "    url_tags = [\"http\", \".com\", \"www.\", \".org\", \".net\", \"&amp\", \"width=\", \"size=\", \"width=\",\n",
    "                \"height=\", \"style=\", \"scrolling=\", \"allowFullScreen=\", \"frameborder=\", \n",
    "                \"allowTransparency=\", \"iframe\", \"&gt\", \"&lt\"]\n",
    "    \n",
    "    filtered_list = [word for word in text_list if any(tag in word for tag in url_tags) == False]\n",
    "    \n",
    "    return \" \".join(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam[\"text\"] = jam[\"text\"].map(lambda x: drop_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"] == \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = jam.drop(jam[jam[\"text\"] == \"\"].index)\n",
    "jam.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 7227\n",
      "\n",
      "training accuracy score: 0.9497251488776912\n",
      " testing accuracy score: 0.8210237031947785\n"
     ]
    }
   ],
   "source": [
    "X = jam[\"text\"]\n",
    "y = jam[\"subreddit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cv, y_train)\n",
    "\n",
    "print(f\"number of features: {len(cv.get_feature_names())}\")\n",
    "print(\"\")\n",
    "print(f\"\"\"training accuracy score: {lr.score(X_train_cv, y_train)}\n",
    " testing accuracy score: {lr.score(X_test_cv, y_test)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...f', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'tvec__max_features': [None], 'tvec__ngram_range': [(1, 1), (1, 2)], 'tvec__stop_words': ['english'], 'svc__C': [10, 100], 'svc__kernel': ['rbf'], 'svc__gamma': ['scale']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = jam[\"text\"]\n",
    "y = jam[\"subreddit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)\n",
    "\n",
    "pipe_tv_sv_2 = Pipeline([\n",
    "    (\"tvec\", TfidfVectorizer()),\n",
    "    (\"svc\", svm.SVC())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    \"tvec__max_features\"    : [None],\n",
    "    \"tvec__ngram_range\"     : [(1,1), (1,2)],\n",
    "    \"tvec__stop_words\"      : [\"english\"],\n",
    "    \"svc__C\"                : [10, 100],\n",
    "    \"svc__kernel\"           : [\"rbf\"],\n",
    "    \"svc__gamma\"            : [\"scale\"]\n",
    "}\n",
    "\n",
    "gs_tv_sv_2 = GridSearchCV(pipe_tv_sv_2, param_grid=pipe_params, cv=5)\n",
    "gs_tv_sv_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 10,\n",
       " 'svc__gamma': 'scale',\n",
       " 'svc__kernel': 'rbf',\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tv_sv_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9147961520842877"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tv_sv_2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275506698728959"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tv_sv_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('nb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'cvec__max_features': [None, 1000, 2000, 5000, 10000], 'cvec__ngram_range': [(1, 1), (1, 2)], 'cvec__stop_words': [None, 'english'], 'nb__alpha': [0, 1, 0.1, 0.5, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = jam[\"text\"]\n",
    "y = jam[\"subreddit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)\n",
    "\n",
    "pipe_cv_nb = Pipeline([\n",
    "    (\"cvec\", CountVectorizer()),\n",
    "    (\"nb\",   MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    \"cvec__max_features\" : [None, 1_000, 2_000, 5_000, 10_000],\n",
    "    \"cvec__ngram_range\"  : [(1,1), (1,2)],\n",
    "    \"cvec__stop_words\"   : [None, \"english\"],\n",
    "    \"nb__alpha\"          : [0, 1, 0.1, 0.5, 5]\n",
    "}\n",
    "\n",
    "gs_cv_nb = GridSearchCV(pipe_cv_nb, param_grid=pipe_params, cv=5)\n",
    "gs_cv_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': None,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': 'english',\n",
       " 'nb__alpha': 0.5}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753779202931745"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv_nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395740295431123"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSI",
   "language": "python",
   "name": "dsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
