{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "# Using NLP to classify posts to one of two subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have scraped the data, I can move into the NLP/modeling phase. First, I'll load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = pd.read_csv(\"./dead.csv\")\n",
    "phish = pd.read_csv(\"./phish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey friends! If you’re heading to Bob Weir and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gratefuldead</td>\n",
       "      <td>1551565481</td>\n",
       "      <td>RollLikeACantaloupe</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UJB 94-02-27</td>\n",
       "      <td>Dagnabbit, I’ve never heard them stretch it ou...</td>\n",
       "      <td>gratefuldead</td>\n",
       "      <td>1551566171</td>\n",
       "      <td>Basil1229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dead On Ice</td>\n",
       "      <td>https://liveforlivemusic.com/news/detroit-red-...</td>\n",
       "      <td>gratefuldead</td>\n",
       "      <td>1551569146</td>\n",
       "      <td>Brightwings73</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Olin Arageed should be suggested more often fo...</td>\n",
       "      <td>Dark stars still the king in my mind but man t...</td>\n",
       "      <td>gratefuldead</td>\n",
       "      <td>1551572220</td>\n",
       "      <td>calliopewoman</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob weir and the wolf bros merch?</td>\n",
       "      <td>While someone’s there can we get a picture of ...</td>\n",
       "      <td>gratefuldead</td>\n",
       "      <td>1551574511</td>\n",
       "      <td>Stratengar</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Hey friends! If you’re heading to Bob Weir and...   \n",
       "1                                       UJB 94-02-27   \n",
       "2                                        Dead On Ice   \n",
       "3  Olin Arageed should be suggested more often fo...   \n",
       "4                  Bob weir and the wolf bros merch?   \n",
       "\n",
       "                                            selftext     subreddit  \\\n",
       "0                                                NaN  gratefuldead   \n",
       "1  Dagnabbit, I’ve never heard them stretch it ou...  gratefuldead   \n",
       "2  https://liveforlivemusic.com/news/detroit-red-...  gratefuldead   \n",
       "3  Dark stars still the king in my mind but man t...  gratefuldead   \n",
       "4  While someone’s there can we get a picture of ...  gratefuldead   \n",
       "\n",
       "   created_utc               author  num_comments  score  is_self   timestamp  \n",
       "0   1551565481  RollLikeACantaloupe             0      3     True  2019-03-02  \n",
       "1   1551566171            Basil1229             0      1     True  2019-03-02  \n",
       "2   1551569146        Brightwings73            10      1     True  2019-03-02  \n",
       "3   1551572220        calliopewoman             8     23     True  2019-03-02  \n",
       "4   1551574511           Stratengar             7      3     True  2019-03-02  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phishy LA (Westside) band needs bass player.</td>\n",
       "      <td>Though we play mostly originals, we sit right ...</td>\n",
       "      <td>phish</td>\n",
       "      <td>1551562683</td>\n",
       "      <td>russellspurlock</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VELVET SEA CHALLANGE..please continue along..</td>\n",
       "      <td>I took a moment from my day\\nAnd wrapped it up...</td>\n",
       "      <td>phish</td>\n",
       "      <td>1551566136</td>\n",
       "      <td>designworksgarage</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Favorite of Mine - Manchester, NH - 10-26-2010</td>\n",
       "      <td>This show hit me in a very special place, and ...</td>\n",
       "      <td>phish</td>\n",
       "      <td>1551567384</td>\n",
       "      <td>Cactus_Bomb</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/30/10 Chalkdust&amp;gt;Whole Lotta Love&amp;gt;Chal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phish</td>\n",
       "      <td>1551571181</td>\n",
       "      <td>save-the-tigers</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anyone else who uses LastFM know if it is poss...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phish</td>\n",
       "      <td>1551578757</td>\n",
       "      <td>ReturnOfTheFox</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       Phishy LA (Westside) band needs bass player.   \n",
       "1      VELVET SEA CHALLANGE..please continue along..   \n",
       "2   A Favorite of Mine - Manchester, NH - 10-26-2010   \n",
       "3  10/30/10 Chalkdust&gt;Whole Lotta Love&gt;Chal...   \n",
       "4  Anyone else who uses LastFM know if it is poss...   \n",
       "\n",
       "                                            selftext subreddit  created_utc  \\\n",
       "0  Though we play mostly originals, we sit right ...     phish   1551562683   \n",
       "1  I took a moment from my day\\nAnd wrapped it up...     phish   1551566136   \n",
       "2  This show hit me in a very special place, and ...     phish   1551567384   \n",
       "3                                                NaN     phish   1551571181   \n",
       "4                                                NaN     phish   1551578757   \n",
       "\n",
       "              author  num_comments  score  is_self   timestamp  \n",
       "0    russellspurlock             6     10     True  2019-03-02  \n",
       "1  designworksgarage             3      0     True  2019-03-02  \n",
       "2        Cactus_Bomb            12     10     True  2019-03-02  \n",
       "3    save-the-tigers             7     14     True  2019-03-02  \n",
       "4     ReturnOfTheFox             0      2     True  2019-03-02  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On first inspection, I notice a few things that I'll address:\n",
    "- null entries in selftext cells\n",
    "- url text\n",
    "- \"\\n\" appearing repeatedly in selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             0\n",
       "selftext        533\n",
       "subreddit         0\n",
       "created_utc       0\n",
       "author            0\n",
       "num_comments      0\n",
       "score             0\n",
       "is_self           0\n",
       "timestamp         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dead.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             0\n",
       "selftext        479\n",
       "subreddit         0\n",
       "created_utc       0\n",
       "author            0\n",
       "num_comments      0\n",
       "score             0\n",
       "is_self           0\n",
       "timestamp         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null values only occur in the selftext column. Since we have text in both the title column and the selftext column, I'll combine these into a single \"text\" column. For the purposes of NLP, there is no need to separate the titles from the selftext, so these null values will naturally get sorted out. This way, we do not need to delete data.\n",
    "\n",
    "First, I'll fill the null entries with a blank string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead.fillna(\"\", inplace=True);\n",
    "phish.fillna(\"\", inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead[\"text\"] = dead[\"title\"] + \" \" + dead[\"selftext\"]\n",
    "phish[\"text\"] = phish[\"title\"] + \" \" + phish[\"selftext\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am going to build an NLP model, the only information that I need is the text column and the subreddit column. I will drop the other columns and then combine the two dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead.drop(columns = [col for col in dead.columns if col not in [\"text\", \"subreddit\"]],\n",
    "          inplace=True)\n",
    "\n",
    "phish.drop(columns = [col for col in phish.columns if col not in [\"text\", \"subreddit\"]],\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = pd.concat([dead, phish], axis=0)\n",
    "jam.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam[\"subreddit\"] = jam[\"subreddit\"].map({\"gratefuldead\": 0,\n",
    "                                         \"phish\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam.to_csv(\"./jam0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of establishing a baseline performance for my models, I am going to save this initial dataframe to a csv and, in a different notebook, go through the process of count vectorizing and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll focus on parsing the text data to remove text that doesn't reflect actual language. For instance, I mentioned above that I noticed recurrences of the substring \"\\n\". This is a symbol for a new paragraph, and so it doesn't pertain to our NLP analysis. Similarly, many of the posts contain links to urls. I will try to delete this type of text, so that I can get close to a dataset that only contains language-related text. I will try to do this with methodology that can be applied generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\\n\" is a substring that seems to be present in several of the posts. How many text entries contain the string, \"\\n\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4165"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(\"\\n\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll replace \"\\n\" with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam[\"text\"] = jam[\"text\"].map(lambda x: x.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [deleted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue with many of the posts is the phrase \"[deleted]\", which denotes text that has retroactively been removed from posts. How many posts contain this substring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(r\"\\[deleted\\]\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll replace \"[deleted]\" with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam[\"text\"] = jam[\"text\"].map(lambda x: re.sub(\"\\[deleted\\]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(r\"\\[deleted\\]\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [removed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, I'll replace the instances of \"[removed]\" with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(r\"\\[removed\\]\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam[\"text\"] = jam[\"text\"].map(lambda x: re.sub(\"\\[removed\\]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jam[jam[\"text\"].str.contains(r\"\\[removed\\]\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam.to_csv(\"./jam1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will break up a string into the individual tokens separated by spaces. By mapping this function to the jam[\"text\"] column, I will filter out urls from the posts. It is possible that some unconventional ones might slip past these filters, but at least I will filter out the vast majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I referred to this stackoverflow page for help on this:\n",
    "# https://stackoverflow.com/questions/8122079/\n",
    "# python-how-to-check-a-string-for-substrings-from-a-list\n",
    "\n",
    "def drop_url(text):\n",
    "    text_list = text.split()\n",
    "    url_tags = [\"http\", \".com\", \"www.\", \".org\", \".net\", \"&amp\", \"width=\", \"size=\", \"width=\",\n",
    "                \"height=\", \"style=\", \"scrolling=\", \"allowFullScreen=\", \"frameborder=\", \n",
    "                \"allowTransparency=\", \"iframe\", \"&gt\", \"&lt\"]\n",
    "    \n",
    "    filtered_list = [word for word in text_list if any(tag in word for tag in url_tags) == False]\n",
    "    \n",
    "    return \" \".join(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam[\"text\"] = jam[\"text\"].map(lambda x: drop_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit text\n",
       "416           0     \n",
       "832           0     \n",
       "8698          1     \n",
       "8701          1     \n",
       "9508          1     "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jam[jam[\"text\"] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = jam.drop(jam[jam[\"text\"] == \"\"].index)\n",
    "jam.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll save the dataframe again, and reassess the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam.to_csv(\"./jam2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to preserve contractions like \"you're\" and \"don't\", so I'll begin by removing punctuation from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"([\\w’]+)\")\n",
    "\n",
    "tokens = [tokenizer.tokenize(text.lower()) for text in jam[\"text\"]]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "tokens_lem = [[lem.lemmatize(i) for i in line] for line in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the list of lemmatized tokens lists, I'll join the lists back into the individual text strings and assign that to the jam[\"text\"] column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jam[\"text\"] = [\" \".join(word_list) for word_list in tokens_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll check for empty cells. Individual symbols or emojis may have been filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit text\n",
       "4456          0     \n",
       "5440          0     \n",
       "7594          1     "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jam[jam[\"text\"] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam = jam.drop(jam[jam[\"text\"] == \"\"].index)\n",
    "jam.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I'll move my focus to trying different models and tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jam.to_csv(\"./jam3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSI",
   "language": "python",
   "name": "dsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
